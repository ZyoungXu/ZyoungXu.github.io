<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ziyang Xu</title>

    <meta name="author" content="Ziyang Xu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon_32.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Ziyang Xu (徐子扬)
                </p>
                <p>I am currently pursuing my third-year Master's degree within the <a href="https://eic.hust.edu.cn/">School of Electronic Information and Communications</a>(EIC) at <a href="https://www.hust.edu.cn/">Huazhong University of Science and Technology</a>(HUST), benefitting from the guidance of Professors <a href="https://xwcv.github.io/">Xinggang Wang</a> and <a href="https://eic.hust.edu.cn/professor/liuwenyu/">Wenyu Liu</a>. Prior to this, I obtained a B.E. degree in Information Engineering from <a href="http://wutinfo.whut.edu.cn/">School of Information Engineering</a>(IE), <a href="https://www.whut.edu.cn/">Wuhan University of Technology</a>(WHUT) in 2022.
                </p>
                <!-- <p>
                  My scholarly interests revolve around the fields of computer vision, artificial general intelligence, and the specialized domain of neuromorphic intelligence.
                </p> -->
                <p style="text-align:center">
                  <a href="mailto:xuzyoung@hust.edu.cn">Email</a> &nbsp;|&nbsp;
                  <a href="https://scholar.google.com/citations?&user=ZYwJrRMAAAAJ">Google Scholar</a> &nbsp;|&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp; -->
				  <!-- <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp; -->
				  <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/ZyoungXu/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/ZiyangXu.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/ZiyangXu_circle.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My scholarly interests revolve around the fields of <strong>Artificial General Intelligence, Computer Vision, Image/Video Generation, Medical Image Analysis</strong>, and <strong>Object Detection/Segmentation</strong>. Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr onmouseout="PixelHacker_stop()" onmouseover="PixelHacker_stop()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='PixelHacker_image'>
                    <img src='images/PixelHacker_after.png' width=100%>
                  </div>
                  <img src='images/PixelHacker_before.png' width=100%>
                </div>
                <script type="text/javascript">
                  function PixelHacker_start() {
                    document.getElementById('PixelHacker_image').style.opacity = "1";
                  }
                  function PixelHacker_stop() {
                    document.getElementById('PixelHacker_image').style.opacity = "0";
                  }
                  PixelHacker_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2504.20438">
                  <span class="papertitle">PixelHacker: Image Inpainting with Structural and Semantic Consistency</span>
                </a>
                <br>
            <strong>Ziyang Xu</strong>,
            <a href="https://scholar.google.com/citations?user=AaQm4aYAAAAJ&hl=zh-CN&oi=ao">Kangsheng Duan</a>,
            Xiaolei Shen,
            Zhifeng Ding,
            <a href="https://eic.hust.edu.cn/professor/liuwenyu/">Wenyu Liu</a>,
            Xiaohu Ruan,
            <a href="https://scholar.google.com/citations?hl=zh-CN&user=SI_oBwsAAAAJ">Xiaoxin Chen</a>,
            <a href="https://xwcv.github.io/">Xinggang Wang</a>
                <br>
                <em><strong>arXiv preprint</strong></em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2504.20438">Paper</a>
                /
                <a href="https://github.com/hustvl/PixelHacker">Code</a>
                /
                <a href="https://hustvl.github.io/projects/PixelHacker">Project Page</a>
                <p></p>
                <p>
                  To address the problem of complex structural and semantic consistency in image inpainting, we propose a novel paradigm called Latent Categories Guidance (LCG), and further propose PixelHacker, a diffusion model-based image inpainting model. PixelHacker efficiently guides the generation process by introducing latent foreground and background representations to achieve structural and semantic consistency, achieving state-of-the-art performance on multiple benchmarks for natural scenes (Places2), face scenes (CelebA-HQ, and FFHQ).
                </p>
              </td>
            </tr>


    <tr onmouseout="GaraMoSt_stop()" onmouseover="GaraMoSt_stop()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='GaraMoSt_image'>
            <img src='images/GaraMoSt_after.png' width=100%>
          </div>
          <img src='images/GaraMoSt_before.png' width=100%>
        </div>
        <script type="text/javascript">
          function GaraMoSt_start() {
            document.getElementById('GaraMoSt_image').style.opacity = "1";
          }

          function GaraMoSt_stop() {
            document.getElementById('GaraMoSt_image').style.opacity = "0";
          }
          GaraMoSt_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2412.14118">
          <span class="papertitle">GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for Efficient Multi-Frame Interpolation in DSA Images</span>
        </a>
        <br>
    <strong>Ziyang Xu</strong>,
    <a href="https://scholar.google.com/citations?user=AaQm4aYAAAAJ&hl=zh-CN&oi=ao">Huangxuan Zhao</a>,
    <a href="https://eic.hust.edu.cn/professor/liuwenyu/">Wenyu Liu</a>,
    <a href="https://xwcv.github.io/">Xinggang Wang</a>
        <br>
        <em><strong>AAAI Conference on Artificial Intelligence (AAAI)</strong></em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2412.14118">Paper</a>
        /
        <a href="https://github.com/ZyoungXu/GaraMoSt">Code</a>
        <p></p>
        <p>
          Compared to our last job MoSt-DSA, GaraMoSt adds multi-granularity motion and structural feature modeling and modifies the overall Pipeline into a highly parallel design, which greatly improves the accuracy and reduces high-frequency and low-frequency noise under the same inference time level (for interpolating 3 frames, only increasing by 0.005s). Comprehensive beyond the SOTA natural scene, and DSA scene methods.
        </p>
      </td>
    </tr>


    <tr onmouseout="XSVID_stop()" onmouseover="XSVID_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='XSVID_image'>
            <img src='images/XSVID_after.png' width=100%>
          </div>
          <img src='images/XSVID_before.png' width=100%>
        </div>
        <script type="text/javascript">
          function XSVID_start() {
            document.getElementById('XSVID_image').style.opacity = "1";
          }

          function XSVID_stop() {
            document.getElementById('XSVID_image').style.opacity = "0";
          }
          XSVID_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2407.18137">
          <span class="papertitle">XS-VID: An Extremely Small Video Object Detection Dataset</span>
        </a>
        <br>
    Jiahao Guo,
    <strong>Ziyang Xu</strong>,
    Lianjun Wu,
    Fei Gao,
    <a href="https://eic.hust.edu.cn/professor/liuwenyu/">Wenyu Liu</a>,
    <a href="https://xwcv.github.io/">Xinggang Wang</a>
        <br>
        <em><strong>arXiv preprint</strong></em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2407.18137">Paper</a>
        /
        <a href="https://github.com/gjhhust/YOLOFT">Code(YOLOFT)</a>
        /
        <a href="https://gjhhust.github.io/XS-VID/">Dataset(XS-VID)</a>
        <p></p>
        <p>
          XS-VID dataset comprises aerial data from various periods and scenes, and extensively collects three types of objects with smaller pixel areas: extremely small (0-12^2), relatively small (12^2-20^2), and generally small (20^2-32^2). XS-VID offers unprecedented breadth and depth in covering and quantifying minuscule objects, significantly enriching the scene and object diversity in the dataset.
          YOLOFT enhances local feature associations and integrates temporal motion features, significantly improving the accuracy and stability of Small Video Object Detection.
        </p>
      </td>
    </tr>


    <tr onmouseout="GenDSA_stop()" onmouseover="GenDSA_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='GenDSA_image'><video  width=100% muted autoplay loop>
          <source src="videos/GenDSAori.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/GenDSA.png' width=100%>
        </div>
        <script type="text/javascript">
          function GenDSA_start() {
            document.getElementById('GenDSA_image').style.opacity = "1";
          }

          function GenDSA_stop() {
            document.getElementById('GenDSA_image').style.opacity = "0";
          }
          GenDSA_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.medj.2024.07.025">
          <span class="papertitle">Large-scale Pretrained Frame Generative Model Enables Real-Time Low-Dose DSA Imaging: an AI System Development and Multicenter Validation Study</span>
        </a>
        <br>
    <a href="https://scholar.google.com/citations?user=AaQm4aYAAAAJ&hl=zh-CN&oi=ao">Huangxuan Zhao*</a>,
    <strong>Ziyang Xu*</strong>,
    Linxia Wu*,
    Lei Chen*,
    <a href="https://github.com/ziwei-cui">Ziwei Cui</a>,
    Jinqiang Ma,
    Tao Sun,
    Yu Lei,
    Nan Wang,
    Hongyao Hu,
    Yiqing Tan,
    Wei Lu,
    Wenzhong Yang,
    Kaibing Liao,
    Gaojun Teng,
    Xiaoyun Liang,
    Yi Li,
    Congcong Feng,
    Xiaoyu Han,
    P.Matthijs van der Sluij,
    Charles B.L.M. Majoie,
    Wim H. van Zwam,
    Yun Feng,
    Theo van Walsum,
    Aad van der Lugt,
    <a href="https://eic.hust.edu.cn/professor/liuwenyu/">Wenyu Liu</a>,
    Xuefeng Kan,
    Ruisheng Su,
    Weihua Zhang,
    <a href="https://xwcv.github.io/">Xinggang Wang</a>,
    <a href="https://scholar.google.com/citations?user=abn37yYAAAAJ&hl=zh-CN&oi=ao">Chuansheng Zheng</a>
        <br>
        * equal contribution
        <br>
        <em><strong>Med (Cell Press)</strong></em>, 2024
        <br>
        <a href="https://www.cell.com/cms/10.1016/j.medj.2024.07.025/attachment/4209f5f8-1a77-4e15-a872-80a6239ec7bd/mmc3.pdf#:~:text=development%20and%20multi-center%20validation%20study,%20Med%20(2024),">Paper</a>
        /
        <a href="https://github.com/ZyoungXu/GenDSA">Code</a>
        <p></p>
        <p>
        GenDSA is a large-scale pretrained multi-frame generative model-based real-time and low-dose DSA imaging system, which pre-trained, fine-tuned and tested on ten million of images from 35 hospitals. Suitable for most DSA scanning protocols, GenDSA could reduce the DSA frame rate (i.e., radiation dose) to 1/3 and generates video that was virtually identical to clinically available protocols. Videos generated by GenDSA reach a comparable level to the full-sampled videos, both in terms of overall quality (4.905 vs 4.935) and lesion assessment (4.825 vs 4.860), which fully demonstrated the potential of GenDSA for clinical applications.
        </p>
      </td>
    </tr>


    <tr onmouseout="MoStDSA_stop()" onmouseover="MoStDSA_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='MoStDSA_image'>
            <img src='images/MoSt-DSA_after.png' width=100%>
          </div>
          <img src='images/MoSt-DSA_before.png' width=100%>
        </div>
        <script type="text/javascript">
          function MoStDSA_start() {
            document.getElementById('MoStDSA_image').style.opacity = "1";
          }

          function MoStDSA_stop() {
            document.getElementById('MoStDSA_image').style.opacity = "0";
          }
          MoStDSA_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2407.07078">
          <span class="papertitle">MoSt-DSA: Modeling Motion and Structural Interactions for Direct Multi-Frame Interpolation in DSA Images</span>
        </a>
        <br>
    <strong>Ziyang Xu</strong>,
    <a href="https://scholar.google.com/citations?user=AaQm4aYAAAAJ&hl=zh-CN&oi=ao">Huangxuan Zhao</a>,
    <a href="https://github.com/ziwei-cui">Ziwei Cui</a>,
    <a href="https://eic.hust.edu.cn/professor/liuwenyu/">Wenyu Liu</a>,
    <a href="https://scholar.google.com/citations?user=abn37yYAAAAJ&hl=zh-CN&oi=ao">Chuansheng Zheng</a>,
    <a href="https://xwcv.github.io/">Xinggang Wang</a>
        <br>
        <em><strong>European Conference on Artificial Intelligence (ECAI)</strong></em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2407.07078">Paper</a>
        /
        <a href="https://github.com/ZyoungXu/MoSt-DSA">Code</a>
        <p></p>
        <p>
        MoSt-DSA is the first work that uses deep learning for DSA frame interpolation, comprehensively achieving SOTA in accuracy, speed, visual effect, and memory usage. Meanwhile, MoSt-DSA is also the first method that directly achieves any number of interpolations at any time steps with just one forward pass during both training and testing. If applied clinically, MoSt-DSA can significantly reduce the DSA radiation dose received by doctors and patients when applied clinically, lowering it by 50%, 67%, and 75% when interpolating 1 to 3 frames, respectively.
        </p>
      </td>
    </tr>

    <!-- <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/smerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/smerf.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function smerf_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function smerf_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          smerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://smerf-3d.github.io/">
          <span class="papertitle">SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration</span>
        </a>
        <br>
		<a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth*</a>,
		<a href="https://phogzone.com/">Peter Hedman*</a>,
		<a href="https://creiser.github.io/">Christian Reiser</a>,
		<a href="">Peter Zhizhin</a>,
		<a href="">Jean-François Thibert</a>,
        <a href="https://lucic.ai/">Mario Lučić</a>,
        <a href="https://szeliski.org/">Richard Szeliski</a>,
		<strong>Jonathan T. Barron</strong>
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="https://smerf-3d.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
      </td>
    </tr> -->


          </tbody></table>


          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody> -->

            <!-- <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr> -->


            <!-- <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr> -->


          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This website is based on <a href="https://github.com/jonbarron/jonbarron_website">jonbarron's template</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
